{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'list_attr_celeba.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2f22f6ad7bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'list_attr_celeba.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipinitialspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'list_attr_celeba.csv'"
     ]
    }
   ],
   "source": [
    "# reading file and partitioning data\n",
    "# TODO: write each data partition to seperate files\n",
    "\n",
    "NUM_TRAINING = 20000\n",
    "NUM_VALIDATION = 5000\n",
    "NUM_TEST = 5000\n",
    "\n",
    "data = [] \n",
    "with open('list_attr_celeba.csv') as Fin:\n",
    "    reader = csv.reader(Fin, skipinitialspace = True, quotechar = \"'\")\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "        \n",
    "# removes feature labels\n",
    "del data[0]\n",
    "\n",
    "training = []\n",
    "validation = []\n",
    "test = []\n",
    "\n",
    "training_label = []\n",
    "validation_label = []\n",
    "test_label = []\n",
    "\n",
    "\n",
    "for i in range(NUM_TRAINING):\n",
    "    index = np.random.randint(0, len(data))\n",
    "    training.append(data.pop(index))\n",
    "    training[i][0] = 1\n",
    "    training_label.append(training[i].pop(21))\n",
    "    \n",
    "for i in range(NUM_VALIDATION):\n",
    "    index = np.random.randint(0, len(data))\n",
    "    validation.append(data.pop(index))\n",
    "    validation[i][0] = 1\n",
    "    validation_label.append(validation[i].pop(21))\n",
    "\n",
    "for i in range(NUM_TEST):\n",
    "    index = np.random.randint(0, len(data))\n",
    "    test.append(data.pop(index))\n",
    "    test[i][0] = 1\n",
    "    test_label.append(test[i].pop(21))\n",
    "\n",
    "    \n",
    "# FEATURE SPACES\n",
    "training = np.array(training)\n",
    "training = training.astype(np.float)\n",
    "\n",
    "validation = np.array(validation)\n",
    "validation = validation.astype(np.float)\n",
    "\n",
    "test = np.array(test)\n",
    "test = test.astype(np.float)\n",
    "\n",
    "\n",
    "# LABEL SPACES\n",
    "training_label = np.array(training_label)\n",
    "training_label = training_label.astype(np.float)\n",
    "\n",
    "validation_label = np.array(validation_label)\n",
    "validation_label = validation_label.astype(np.float)\n",
    "\n",
    "test_label = np.array(test_label)\n",
    "test_label = test_label.astype(np.float)\n",
    "\n",
    "\n",
    "# print(training_label.shape)\n",
    "# print(validation_label.shape)\n",
    "# print(test_label.shape)\n",
    "# print(training.shape)\n",
    "# print(validation.shape)\n",
    "# print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER FUNCTIONS\n",
    "\n",
    "def logistic_regression_SGD(data, label, max_iter, learning_rate): \n",
    "    '''\n",
    "    The logistic regression classifier function\n",
    "    using Sochastic Gradient Descent.\n",
    "\n",
    "    Args:\n",
    "    data: train data with shape (20000, 40), which means 20000 samples and \n",
    "          each sample has 40 features\n",
    "    label: train data's label with shape (20000,1). \n",
    "           1 for male and -1 for female\n",
    "    max_iter: max iteration numbers\n",
    "    learning_rate: learning rate for weight update\n",
    "    Returns:\n",
    "        w: the seperater with shape (40, 1). You must initilize it with w = np.zeros((d,1))\n",
    "    '''\n",
    "    N = len(data)\n",
    "    d = len(data[0])\n",
    "    # initialize w0\n",
    "    w = np.zeros((d, 1))\n",
    "    w = np.transpose(w)\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        # pick random point\n",
    "        i = np.random.randint(0, N)\n",
    "        # calculate gradient\n",
    "        gradient = (-label[i] * data[i]) / (1 + np.exp(label[i] * w * data[i]))\n",
    "        # update weights\n",
    "        w -= (learning_rate * gradient)\n",
    "\n",
    "    return np.transpose(w)\n",
    "\n",
    "\n",
    "\n",
    "def logistic_regression(data, label, max_iter, learning_rate):\n",
    "    '''\n",
    "    The logistic regression classifier function.\n",
    "\n",
    "    Args:\n",
    "    data: train data with shape (1561, 3), which means 1561 samples and \n",
    "          each sample has 3 features.(1, symmetry, average internsity)\n",
    "    label: train data's label with shape (1561,1). \n",
    "           1 for digit number 1 and -1 for digit number 5.\n",
    "    max_iter: max iteration numbers\n",
    "    learning_rate: learning rate for weight update\n",
    "\n",
    "    Returns:\n",
    "        w: the seperater with shape (3, 1). You must initilize it with w = np.zeros((d,1))\n",
    "    '''\n",
    "    N = len(data)\n",
    "    d = len(data[0])\n",
    "    # initialize w0\n",
    "    w = np.zeros((d, 1))\n",
    "    w = np.transpose(w)\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        # calculate gradient\n",
    "        gradientSum = np.zeros((1, d))\n",
    "        for n in range(N):\n",
    "            gradientSum += ( (label[n] * data[n]) / (1 + np.exp(label[n]* np.dot(w, data[n]))) )\n",
    "        gradient = (-1/N) * gradientSum\n",
    "        # update weights \n",
    "        w -= (learning_rate *gradient)\n",
    "\n",
    "    return np.transpose(w)\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(x, y, w):\n",
    "    '''\n",
    "    This function is used to compute accuracy of a logsitic regression model.\n",
    "\n",
    "    Args:\n",
    "    x: input data with shape (n, d), where n represents total data samples and d represents\n",
    "        total feature numbers of a certain data sample.\n",
    "    y: corresponding label of x with shape(n, 1), where n represents total data samples.\n",
    "    w: the seperator learned from logistic regression function with shape (d, 1),\n",
    "        where d represents the total feature numbers of a certain data sample.\n",
    "\n",
    "    Return \n",
    "    accuracy: total percentage of correctly classified samples. Set the threshold as 0.5,\n",
    "    which means, if the predicted probability > 0.5, classify as 1; Otherwise, classify as -1.\n",
    "    mistakes = 0\n",
    "    '''\n",
    "    mistakes = 0\n",
    "    n = len(y)\n",
    "    w = np.transpose(w)\n",
    "    for z in range(n):\n",
    "        y_pred = 1.0 if sigmoid(np.dot(w,x[z])) > .5 else -1.0\n",
    "        if(y_pred != y[z]):\n",
    "            mistakes += 1\n",
    "        \n",
    "    return (n-mistakes)/n\n",
    "\n",
    "\n",
    "def sigmoid(s):\n",
    "    return (1/(1 + np.exp(-s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING WITH SGD\n",
      "\n",
      "\n",
      "max iteration testcase0: Train accuracy: 0.848150, Test accuracy: 0.844300\n",
      "max iteration testcase1: Train accuracy: 0.658600, Test accuracy: 0.660000\n",
      "max iteration testcase2: Train accuracy: 0.845400, Test accuracy: 0.844300\n",
      "max iteration testcase3: Train accuracy: 0.796300, Test accuracy: 0.796050\n",
      "learning rate testcase0: Train accuracy: 0.628050, Test accuracy: 0.630750\n",
      "learning rate testcase1: Train accuracy: 0.667550, Test accuracy: 0.670000\n",
      "learning rate testcase2: Train accuracy: 0.612250, Test accuracy: 0.614750\n",
      "\n",
      "TESTING LOGISTIC REGRESSION\n",
      "\n",
      "\n",
      "max iteration testcase0: Train accuracy: 0.931550, Test accuracy: 0.930650\n",
      "max iteration testcase1: Train accuracy: 0.932850, Test accuracy: 0.930650\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5f685db66583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTESTING LOGISTIC REGRESSION\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mAin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max iteration testcase%d: Train accuracy: %f, Test accuracy: %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-4b6233f7714c>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(data, label, max_iter, learning_rate)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mgradientSum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mgradientSum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradientSum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TESTING \n",
    "learning_rate = [.1, .2, .5]\n",
    "max_iter = [500, 1000, 10000, 100000]\n",
    "\n",
    "\n",
    "print('TESTING WITH SGD\\n')\n",
    "for i, m_iter in enumerate(max_iter):\n",
    "    w = logistic_regression_SGD(training, training_label, m_iter, learning_rate[1])\n",
    "    Ain, Aout = accuracy(training, training_label, w), accuracy(test, test_label, w)\n",
    "    print(\"max iteration testcase%d: Train accuracy: %f, Test accuracy: %f\"%(i, Ain, Aout))\n",
    "\n",
    "for i, l_rate in enumerate(learning_rate):\n",
    "    w = logistic_regression_SGD(training, training_label, max_iter[3], l_rate)\n",
    "    Ain, Aout = accuracy(training, training_label, w), accuracy(test, test_label, w)\n",
    "    print(\"learning rate testcase%d: Train accuracy: %f, Test accuracy: %f\"%(i, Ain, Aout))\n",
    "    \n",
    "\n",
    "    \n",
    "print('\\n\\nTESTING LOGISTIC REGRESSION\\n')\n",
    "for i, m_iter in enumerate(max_iter):\n",
    "    w = logistic_regression(training, training_label, m_iter, learning_rate[1])\n",
    "    Ain, Aout = accuracy(training, training_label, w), accuracy(test, test_label, w)\n",
    "    print(\"max iteration testcase%d: Train accuracy: %f, Test accuracy: %f\"%(i, Ain, Aout))\n",
    "\n",
    "for i, l_rate in enumerate(learning_rate):\n",
    "    w = logistic_regression(training, training_label, max_iter[3], l_rate)\n",
    "    Ain, Aout = accuracy(training, training_label, w), accuracy(test, test_label, w)\n",
    "    print(\"learning rate testcase%d: Train accuracy: %f, Test accuracy: %f\"%(i, Ain, Aout))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
